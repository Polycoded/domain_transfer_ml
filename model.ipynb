{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71686644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "FRAMES = 16\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for label, cls in enumerate([\"fake\", \"real\"]):\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            for video in sorted(os.listdir(cls_path)):\n",
    "                self.samples.append((os.path.join(cls_path, video), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "        frame_files = sorted(os.listdir(video_path))\n",
    "\n",
    "        # hard assertion — fail fast, not silently\n",
    "        assert len(frame_files) == FRAMES, f\"{video_path} has {len(frame_files)} frames\"\n",
    "\n",
    "        frames = []\n",
    "        for f in frame_files:\n",
    "            img = Image.open(os.path.join(video_path, f)).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "\n",
    "        x = torch.stack(frames)  # [16, 3, 224, 224]\n",
    "        y = torch.tensor(label).long()\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c90480b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93845ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 3, 224, 224])\n",
      "tensor([0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "ROOT = r\"J:\\Chapter\\IEEE-CS\\frames_new\\train\"\n",
    "\n",
    "dataset = VideoFrameDataset(ROOT, transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# sanity check — if this fails, stop\n",
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)  # MUST be [4, 16, 3, 224, 224]\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b68d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broken videos: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ROOT = r\"J:\\Chapter\\IEEE-CS\\frames_new\\train\"\n",
    "BAD = []\n",
    "\n",
    "for cls in [\"fake\", \"real\"]:\n",
    "    cls_path = os.path.join(ROOT, cls)\n",
    "    for vid in os.listdir(cls_path):\n",
    "        vid_path = os.path.join(cls_path, vid)\n",
    "        n = len(os.listdir(vid_path))\n",
    "        if n != 16:\n",
    "            BAD.append((vid_path, n))\n",
    "\n",
    "print(\"Broken videos:\", len(BAD))\n",
    "for b in BAD[:10]:\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27db480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class VideoMaxPoolModel(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Frame-level feature extractor\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # remove FC\n",
    "        self.feature_dim = resnet.fc.in_features\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(self.feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, 3, 224, 224]  (B=batch, T=16 frames)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B * T, C, H, W)           # treat frames as batch\n",
    "        feats = self.cnn(x)                  # [B*T, feat,1,1]\n",
    "        feats = feats.view(B, T, -1)         # [B, T, feat]\n",
    "        feats, _ = torch.max(feats, dim=1)   # temporal max pooling\n",
    "        out = self.fc(feats)                 # [B, 1]\n",
    "        return torch.sigmoid(out).squeeze(1) # [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c9347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\johnp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.8242 | Val Acc: 0.571\n",
      "Epoch 2/10 | Loss: 0.4811 | Val Acc: 0.411\n",
      "Epoch 3/10 | Loss: 0.3874 | Val Acc: 0.375\n",
      "Epoch 4/10 | Loss: 0.3386 | Val Acc: 0.571\n",
      "Epoch 5/10 | Loss: 0.3446 | Val Acc: 0.446\n",
      "Epoch 6/10 | Loss: 0.3036 | Val Acc: 0.518\n",
      "Epoch 7/10 | Loss: 0.2748 | Val Acc: 0.482\n",
      "Epoch 8/10 | Loss: 0.2405 | Val Acc: 0.518\n",
      "Epoch 9/10 | Loss: 0.1785 | Val Acc: 0.518\n",
      "Epoch 10/10 | Loss: 0.1880 | Val Acc: 0.446\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VideoMaxPoolModel(pretrained=True).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            out_val = model(x_val) > 0.5\n",
    "            correct += (out_val == y_val).sum().item()\n",
    "\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Val Acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08dd218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model saved as 'deepfake_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS,\n",
    "    'val_acc': val_acc\n",
    "}, 'deepfake_model.pth')\n",
    "\n",
    "print(\"\\n✅ Model saved as 'deepfake_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1c414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
